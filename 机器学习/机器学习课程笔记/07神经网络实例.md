## 假设函数

* 神经网络本身，即是假设函数能够计算输入相对的输出。

## 代价函数

* $L$表示神经网络的总层数。
* $s_L$表示第L层单元的个数。
* $K$表示输出层单元的个数
* 代价函数相当于第i组数据输入时，产生的误差。

$$
J(\theta)=-\frac{1}{m}[\sum_i^my^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log (1-h_\theta (x^{(i)}))]+\frac{\lambda}{2m}\sum_1^n\theta_j^2
$$
* $a^{(i)}$表示第i层的单元值。
* $\Theta^{(i)}$第i层的权重
* $z^{(i)}$第i层的加权值
* $\delta^{(i)}$第i层的反向传播误差。

## 最小化代价函数：反向传播算法


* 在这里的上标，代表的不是输入的代数（即第几次迭代），而是神经网络的层数。下标表示的是神经网络某层的单元数。
* 原理：神经网络的值会随着假设函数正向传播。神经网络的误差会随着假设函数反向传播到第二层。利用每一层的单元值和神经网络的误差能够计算每一层的梯度下降向量，通过梯度下降向量，完成参数的更新。
* 神经网络的正向传播过程

![](../img/forward.png)

* 神经网络的反向传播过程

![](../img/backward.png)

* 神经网络反向传播算法实现

![](../img/backwark_过程.png)


## 反向传播算法理解

* $\delta_j^{(i)}$表示第i层的单元j的误差。他相当于单元j的代价函数$J=cost_j$关于加权值$z^{(i)}_j$的偏导数

$$
\delta_j^{(i)}=\frac{\partial}{\partial z}cost_j
$$


![](../img/backward_原理.png)

## 梯度检测

* 使用差分方法，近似某个点的梯度。普通的梯度是通过求导公式得到导数，然后进行梯度下降。可以使用差分近似导数，与梯度进行对比，完成梯度检测。

## 权重随机初始化

* 避免权重相同，出现高度冗余。
* 因为在梯度下降更新过程中，相同的权重，会进行相同程度的更新。
* 打破对称性流程

