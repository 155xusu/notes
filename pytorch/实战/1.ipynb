{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "38740d3277777e2cd7c6c2cc9d8addf5118fdf3f82b1b39231fd12aeac8aee8b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 3 Tensor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 4]) torch.float32 cpu\ntensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])\ntensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\ntensor([[[1., 0., 1., 1.]],\n\n        [[1., 0., 1., 1.]],\n\n        [[1., 0., 1., 1.]],\n\n        [[1., 0., 1., 1.]]])\ntensor([[6., 5., 6., 6.],\n        [6., 5., 6., 6.],\n        [6., 5., 6., 6.],\n        [6., 5., 6., 6.]])\n[[6. 5. 6. 6.]\n [6. 5. 6. 6.]\n [6. 5. 6. 6.]\n [6. 5. 6. 6.]]\n[[9. 8. 9. 9.]\n [9. 8. 9. 9.]\n [9. 8. 9. 9.]\n [9. 8. 9. 9.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "# 张量初始化的方法\n",
    "# 直接生成张量\n",
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "\n",
    "# numpy数组转化\n",
    "np_array = np.array(data)\n",
    "x_np=torch.from_numpy(np_array)\n",
    "\n",
    "# 通过已有的张量生成新的张量\n",
    "x_ones = torch.ones_like(x_data)\n",
    "x_rand = torch.rand_like(x_data,dtype = torch.float)\n",
    "\n",
    "# 通过制定数组的维度生成张量\n",
    "shape = (2,3)\n",
    "rand_tensor = torch.rand(size=shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "# 张量的属性\n",
    "tensor = torch.rand(3,4)\n",
    "print(tensor.shape,tensor.dtype,tensor.device)\n",
    "\n",
    "# 张量运算\n",
    "# 将tensor导入gpu内运行\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "# 张量的索引和切片-与numpy数组完全一致\n",
    "tensor = torch.ones(4,4)\n",
    "tensor[:,1]=0\n",
    "print(tensor)\n",
    "\n",
    "# 张量的拼接_在某个内层已有的维度上延伸\n",
    "t1 = torch.cat([tensor,tensor,tensor],dim=1)\n",
    "print(t1)\n",
    "\n",
    "# 张量的拼接_创建一个新的维度，组装原来的tensor。在某个维度上组装原来的tensor。原来的tensor不会被破坏\n",
    "t2 = torch.stack((tensor,),dim=1)\n",
    "print(t2)\n",
    "\n",
    "# 张量的乘积和矩阵乘法\n",
    "# 逐个元素相乘\n",
    "tensor.mul(tensor)\n",
    "tensor*tensor\n",
    "# 矩阵乘法\n",
    "tensor.matmul(tensor.T)\n",
    "tensor @ tensor.T\n",
    "\n",
    "# 自赋值运算 _自赋值。否则返回运算后的值。\n",
    "tensor.add_(5)\n",
    "print(tensor)\n",
    "\n",
    "# tensor与numpy的相互转换。而且numpy与tensor公用同样的地址的值\n",
    "np_array = tensor.numpy()\n",
    "print(np_array)\n",
    "tensor.add_(3)\n",
    "print(np_array)"
   ]
  },
  {
   "source": [
    "## 4 torch.autograd "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正向传播和方向传播的用法\n",
    "import torch,torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "data = torch.rand(1,3,64,64)\n",
    "labels = torch.rand(1,1000)\n",
    "# print(data)\n",
    "prediction = model(data)\n",
    "# print(prediction.type)\n",
    "loss = (prediction-labels).sum()\n",
    "loss.backward()\n",
    "optim = torch.optim.SGD(model.parameters(),lr=1e-2,momentum=0.9)\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\nNone\ntensor([36., 81.])\ntensor([-12.,  -8.])\ntensor([True, True])\ntensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# autograd实现微分\n",
    "# tensor 能够通过autograd自动梯度。进行梯度下降分析。即记住运算过程。\n",
    "# 每个张量都有自己的grad属性。用来存储本张量的梯度下降值。\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([2.,3.],requires_grad=True)\n",
    "b = torch.tensor([6.,4.],requires_grad=True)\n",
    "\n",
    "\n",
    "Q = 3*a**3 -b**2\n",
    "\n",
    "# 构建一个梯度运算的图（树）。\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "\n",
    "# Q.sum().backward()\n",
    "external_grad = torch.tensor([1.,1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "source": [
    "> 计算图.自动维护一个计算图。输入节点为叶节点。输出节点为根节点。的树。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5 神经网络"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.conv1 = nn.Conv2d(1,6,3)\n",
    "        self.conv2 = nn.Conv2d(6,16,3)\n",
    "        self.fc1 = nn.Linear(16*6*6,120)\n",
    "        self"
   ]
  }
 ]
}