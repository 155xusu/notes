{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 联合学习设置\n",
    "对于使用TrainConfig的联合学习设置，我们需要不同的参与者：\n",
    "\n",
    "* 工作者：自己的数据集。\n",
    "\n",
    "* 协调员：知道工作人员以及每个工作人员中存在的数据集名称的实体。\n",
    "\n",
    "* 评估器：保存测试数据并跟踪模型性能\n",
    "\n",
    "每个工作进程由两部分表示，即调度程序本地的代理（Websocket客户端工作进程）和保存数据并执行计算的远程实例。远程部分称为Websocket服务器工作程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# inspect模块主要用来查看相关的代码。可以显示源代码。\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 准备工作：启动WebSocket的worker\n",
    "\n",
    "因此，首先，我们需要创建远程工作者。为此，您需要在终端中运行（无法从笔记本计算机上运行）：\n",
    "\n",
    "python start_websocket_servers.py\n",
    "\n",
    "\n",
    "这是怎么回事？\n",
    "该脚本将实例化三个工作人员Alice，Bob和Charlie并准备他们的本地数据。每个工作人员都设置为拥有MNIST培训数据集的子集。爱丽丝持有与数字0-3对应的所有图像，鲍勃持有与数字4-6对应的所有图像，查理持有与数字7-9对应的所有图像。\n",
    "\n",
    "工人\t本地数据集中的数字\t样品数\n",
    "爱丽丝\t0-3\t24754\n",
    "鲍勃\t4-6\t17181\n",
    "查理\t7-9\t18065\n",
    "\n",
    "| Worker      | Digits in local dataset | Number of samples |\n",
    "| ----------- | ----------------------- | ----------------- |\n",
    "| Alice       | 0-3                     | 24754             |\n",
    "| Bob         | 4-6                     | 17181             |\n",
    "| Charlie     | 7-9                     | 18065             |\n",
    "\n",
    "\n",
    "该评估程序将称为“测试”，并保存整个MNIST测试数据集。\n",
    "\n",
    "| Evaluator   | Digits in local dataset | Number of samples |\n",
    "| ----------- | ----------------------- | ----------------- |\n",
    "| Testing     | 0-9                     | 10000             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "def start_websocket_server_worker(id, host, port, hook, verbose, keep_labels=None, training=True):\n    \"\"\"Helper function for spinning up a websocket server and setting up the local datasets.\"\"\"\n\n    server = websocket_server.WebsocketServerWorker(\n        id=id, host=host, port=port, hook=hook, verbose=verbose\n    )\n\n    # Setup toy data (mnist example)\n    mnist_dataset = datasets.MNIST(\n        root=\"../../官方教程/data\",\n        train=training,\n        download=True,\n        transform=transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n        ),\n    )\n\n    if training:\n        indices = np.isin(mnist_dataset.targets, keep_labels).astype(\"uint8\")\n        logger.info(\"number of true indices: %s\", indices.sum())\n        selected_data = (\n            torch.native_masked_select(mnist_dataset.data.transpose(0, 2), torch.tensor(indices))\n            .view(28, 28, -1)\n            .transpose(2, 0)\n        )\n        logger.info(\"after selection: %s\", selected_data.shape)\n        selected_targets = torch.native_masked_select(mnist_dataset.targets, torch.tensor(indices))\n\n        dataset = sy.BaseDataset(\n            data=selected_data, targets=selected_targets, transform=mnist_dataset.transform\n        )\n        key = \"mnist\"\n    else:\n        dataset = sy.BaseDataset(\n            data=mnist_dataset.data,\n            targets=mnist_dataset.targets,\n            transform=mnist_dataset.transform,\n        )\n        key = \"mnist_testing\"\n\n    server.add_dataset(dataset, key=key)\n    count = [0] * 10\n    logger.info(\n        \"MNIST dataset (%s set), available numbers on %s: \", \"train\" if training else \"test\", id\n    )\n    for i in range(10):\n        count[i] = (dataset.targets == i).sum().item()\n        logger.info(\"      %s: %s\", i, count[i])\n\n    logger.info(\"datasets: %s\", server.datasets)\n    if training:\n        logger.info(\"len(datasets[mnist]): %s\", len(server.datasets[key]))\n\n    server.start()\n    return server\n\n"
     ]
    }
   ],
   "source": [
    "import run_websocket_server\n",
    "# 用来查看模块内部的代码。\n",
    "print(inspect.getsource(run_websocket_server.start_websocket_server_worker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在继续之前，我们首先需要导入依赖项，设置所需的参数并配置日志记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "import sys\n",
    "# python中异步IO的实现方法。提供了websocket一种应用层全双工的异步、非阻塞通信方式，通过消息响应实现通信。\n",
    "import asyncio\n",
    "\n",
    "# syft模块主要封装实现了基于websocket的异步通信。\n",
    "import syft as sy\n",
    "from syft.workers.websocket_client import WebsocketClientWorker\n",
    "from syft.frameworks.torch.fl import utils\n",
    "\n",
    "# torch主要提供了机器学习的算法。\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "# rwc提供了客户端运行的主要方法。\n",
    "import run_websocket_client as rwc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将syft与torch建立联系\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Namespace(batch_size=32, cuda=False, federate_after_n_batches=10, lr=0.1, save_model=False, seed=1, test_batch_size=128, training_rounds=40, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# 配置训练过程中的相关列参数。\n",
    "# batch_size batch大小\n",
    "# cuda 是否启用GPU\n",
    "# federate_after_n_batches多少轮之后进行联邦平均\n",
    "# lr学习率\n",
    "# test_batch_size测试数据集\n",
    "# training_round worker上训练的次数。\n",
    "# verbose 概要？用来做什么的不清楚。\n",
    "\n",
    "args = rwc.define_and_get_arguments(args=[])\n",
    "use_cuda = args.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置一个日志模块。使用python原本的logging模块。\n",
    "import logging\n",
    "\n",
    "# 获得一个命名的记录器\n",
    "logger = logging.getLogger(\"run_websocket_client\")\n",
    "\n",
    "if not len(logger.handlers):\n",
    "    # print(123)\n",
    "    FORMAT = \"%(asctime)s - %(message)s\"\n",
    "    DATE_FMT = \"%H:%M:%S\"\n",
    "    formatter = logging.Formatter(FORMAT, DATE_FMT)\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.propagate = False\n",
    "LOG_LEVEL = logging.DEBUG\n",
    "logger.setLevel(LOG_LEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们实例化websocket客户端工作程序，即远程工作程序的本地代理。请注意，如果websocket服务器工作程序未在运行，则此步骤将失败。\n",
    "\n",
    "工人Alice，Bob和Charlie将进行培训，然后由测试人员托管测试数据并进行评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在客户端定义服务端的句柄。通过websocketclientworker类，建立通信。每一个类维护一个通信链接。\n",
    "# 将客户端websocket与启动worker服务端的websocket建立一对一链接。\n",
    "# pysyft通过设置，将通信模块单独剥离出来。\n",
    "kwargs_websocket = {\"host\": \"127.0.0.1\", \"hook\": hook, \"verbose\": args.verbose}\n",
    "alice = WebsocketClientWorker(id=\"alice\", port=8777, **kwargs_websocket)\n",
    "bob = WebsocketClientWorker(id=\"bob\", port=8778, **kwargs_websocket)\n",
    "charlie = WebsocketClientWorker(id=\"charlie\", port=8779, **kwargs_websocket)\n",
    "testing = WebsocketClientWorker(id=\"testing\", port=8780, **kwargs_websocket)\n",
    "\n",
    "# 用来试下通信的句柄。\n",
    "worker_instances = [alice, bob, charlie]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 设置培训\n",
    "\n",
    "让我们实例化机器学习模型。这是一个具有2个卷积层和2个完全连接层的小型神经网络。它使用ReLU激活和最大池化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n"
     ]
    }
   ],
   "source": [
    "# 输出模型。\n",
    "print(inspect.getsource(rwc.Net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=800, out_features=500, bias=True)\n  (fc2): Linear(in_features=500, out_features=10, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "model = rwc.Net().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使模型可序列化\n",
    "为了将模型发送给工作人员，我们需要模型可序列化，为此我们使用jit。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将需要训练的模型进行序列化。\n",
    "# jit提供了一种不依赖Python环境的执行方法。这样在发送到客户端之后，即是没有导入相关的包。也能运行模型，进行梯度下降。\n",
    "traced_model = torch.jit.trace(model, torch.zeros([1, 1, 28, 28], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3  让我们开始训练\n",
    "现在我们准备开始联合培训。我们将分别对每个工人进行给定数量的批次培训，然后计算所得模型的联合平均值。\n",
    "\n",
    "每隔10轮培训，我们将评估工人返回的模型以及通过联合平均获得的模型的性能。\n",
    "\n",
    "性能将作为准确性（正确预测的比率）和预测数字的直方图给出。这很有趣，因为每个工人仅拥有数字的一个子集。因此，在开始时，每个工作人员将仅预测他们的人数，并且仅通过联合平均过程知道其他人数。\n",
    "\n",
    "培训以异步方式完成。这意味着调度程序仅告诉工人进行培训，而不会阻止与下一个工人交谈之前等待培训的结果。\n",
    "\n",
    "训练的参数在参数中给出。每个工作人员将按照给定数量的批次进行培训，该数量由federate_after_n_batches的值给出。还配置了培训批次大小和学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Federate_after_n_batches: 10\nBatch size: 32\nInitial learning rate: 0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Federate_after_n_batches: \" + str(args.federate_after_n_batches))\n",
    "print(\"Batch size: \" + str(args.batch_size))\n",
    "print(\"Initial learning rate: \" + str(args.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "20:21:34 - Training round 1/40\n",
      "20:21:42 - Evaluating models\n",
      "20:21:45 - Model update alice: Percentage numbers 0-3: 100%, 4-6: 0%, 7-9: 0%\n",
      "20:21:45 - Model update alice: Average loss: 0.0216, Accuracy: 1498/10000 (14.98%)\n",
      "20:21:49 - Model update bob: Percentage numbers 0-3: 0%, 4-6: 100%, 7-9: 0%\n",
      "20:21:49 - Model update bob: Average loss: 0.0441, Accuracy: 892/10000 (8.92%)\n",
      "20:21:52 - Model update charlie: Percentage numbers 0-3: 0%, 4-6: 0%, 7-9: 100%\n",
      "20:21:52 - Model update charlie: Average loss: 0.0323, Accuracy: 1092/10000 (10.92%)\n",
      "20:21:56 - Federated model: Percentage numbers 0-3: 0%, 4-6: 99%, 7-9: 0%\n",
      "20:21:56 - Federated model: Average loss: 0.0177, Accuracy: 892/10000 (8.92%)\n",
      "20:21:56 - Training round 2/40\n",
      "20:22:02 - Training round 3/40\n",
      "20:22:10 - Training round 4/40\n",
      "20:22:17 - Training round 5/40\n",
      "20:22:24 - Training round 6/40\n",
      "20:22:32 - Training round 7/40\n",
      "20:22:39 - Training round 8/40\n",
      "20:22:46 - Training round 9/40\n",
      "20:22:53 - Training round 10/40\n",
      "20:23:00 - Training round 11/40\n",
      "20:23:07 - Evaluating models\n",
      "20:23:10 - Model update alice: Percentage numbers 0-3: 80%, 4-6: 13%, 7-9: 5%\n",
      "20:23:10 - Model update alice: Average loss: 0.0106, Accuracy: 5668/10000 (56.68%)\n",
      "20:23:14 - Model update bob: Percentage numbers 0-3: 17%, 4-6: 73%, 7-9: 8%\n",
      "20:23:14 - Model update bob: Average loss: 0.0133, Accuracy: 5320/10000 (53.20%)\n",
      "20:23:17 - Model update charlie: Percentage numbers 0-3: 11%, 4-6: 2%, 7-9: 86%\n",
      "20:23:17 - Model update charlie: Average loss: 0.0192, Accuracy: 4246/10000 (42.46%)\n",
      "20:23:20 - Federated model: Percentage numbers 0-3: 42%, 4-6: 21%, 7-9: 36%\n",
      "20:23:20 - Federated model: Average loss: 0.0030, Accuracy: 8647/10000 (86.47%)\n",
      "20:23:20 - Training round 12/40\n",
      "20:23:27 - Training round 13/40\n",
      "20:23:34 - Training round 14/40\n",
      "20:23:41 - Training round 15/40\n",
      "20:23:47 - Training round 16/40\n",
      "20:23:54 - Training round 17/40\n",
      "20:24:01 - Training round 18/40\n",
      "20:24:08 - Training round 19/40\n",
      "20:24:15 - Training round 20/40\n",
      "20:24:22 - Training round 21/40\n",
      "20:24:29 - Evaluating models\n",
      "20:24:32 - Model update alice: Percentage numbers 0-3: 70%, 4-6: 16%, 7-9: 13%\n",
      "20:24:32 - Model update alice: Average loss: 0.0092, Accuracy: 6886/10000 (68.86%)\n",
      "20:24:36 - Model update bob: Percentage numbers 0-3: 28%, 4-6: 59%, 7-9: 11%\n",
      "20:24:36 - Model update bob: Average loss: 0.0072, Accuracy: 6739/10000 (67.39%)\n",
      "20:24:39 - Model update charlie: Percentage numbers 0-3: 31%, 4-6: 8%, 7-9: 59%\n",
      "20:24:39 - Model update charlie: Average loss: 0.0072, Accuracy: 6914/10000 (69.14%)\n",
      "20:24:43 - Federated model: Percentage numbers 0-3: 44%, 4-6: 26%, 7-9: 28%\n",
      "20:24:43 - Federated model: Average loss: 0.0017, Accuracy: 9354/10000 (93.54%)\n",
      "20:24:43 - Training round 22/40\n",
      "20:24:50 - Training round 23/40\n",
      "20:24:56 - Training round 24/40\n",
      "20:25:03 - Training round 25/40\n",
      "20:25:10 - Training round 26/40\n",
      "20:25:17 - Training round 27/40\n",
      "20:25:23 - Training round 28/40\n",
      "20:25:30 - Training round 29/40\n",
      "20:25:37 - Training round 30/40\n",
      "20:25:44 - Training round 31/40\n",
      "20:25:51 - Evaluating models\n",
      "20:25:54 - Model update alice: Percentage numbers 0-3: 54%, 4-6: 24%, 7-9: 21%\n",
      "20:25:54 - Model update alice: Average loss: 0.0035, Accuracy: 8525/10000 (85.25%)\n",
      "20:25:57 - Model update bob: Percentage numbers 0-3: 34%, 4-6: 49%, 7-9: 16%\n",
      "20:25:57 - Model update bob: Average loss: 0.0050, Accuracy: 7755/10000 (77.55%)\n",
      "20:26:01 - Model update charlie: Percentage numbers 0-3: 32%, 4-6: 13%, 7-9: 54%\n",
      "20:26:01 - Model update charlie: Average loss: 0.0059, Accuracy: 7441/10000 (74.41%)\n",
      "20:26:04 - Federated model: Percentage numbers 0-3: 41%, 4-6: 28%, 7-9: 29%\n",
      "20:26:04 - Federated model: Average loss: 0.0012, Accuracy: 9572/10000 (95.72%)\n",
      "20:26:04 - Training round 32/40\n",
      "20:26:11 - Training round 33/40\n",
      "20:26:18 - Training round 34/40\n",
      "20:26:25 - Training round 35/40\n",
      "20:26:32 - Training round 36/40\n",
      "20:26:38 - Training round 37/40\n",
      "20:26:45 - Training round 38/40\n",
      "20:26:52 - Training round 39/40\n",
      "20:26:59 - Training round 40/40\n",
      "20:27:06 - Evaluating models\n",
      "20:27:09 - Model update alice: Percentage numbers 0-3: 54%, 4-6: 23%, 7-9: 21%\n",
      "20:27:09 - Model update alice: Average loss: 0.0036, Accuracy: 8535/10000 (85.35%)\n",
      "20:27:13 - Model update bob: Percentage numbers 0-3: 36%, 4-6: 43%, 7-9: 19%\n",
      "20:27:13 - Model update bob: Average loss: 0.0035, Accuracy: 8318/10000 (83.18%)\n",
      "20:27:16 - Model update charlie: Percentage numbers 0-3: 31%, 4-6: 12%, 7-9: 55%\n",
      "20:27:16 - Model update charlie: Average loss: 0.0057, Accuracy: 7364/10000 (73.64%)\n",
      "20:27:19 - Federated model: Percentage numbers 0-3: 41%, 4-6: 28%, 7-9: 30%\n",
      "20:27:19 - Federated model: Average loss: 0.0010, Accuracy: 9631/10000 (96.31%)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = args.lr\n",
    "device = \"cpu\"  #torch.device(\"cpu\")\n",
    "traced_model = torch.jit.trace(model, torch.zeros([1, 1, 28, 28], dtype=torch.float))\n",
    "for curr_round in range(1, args.training_rounds + 1):\n",
    "    logger.info(\"Training round %s/%s\", curr_round, args.training_rounds)\n",
    "\n",
    "    # 异步调用多个客户端执行并行训练。await等待多个异步调用执行完成。\n",
    "    # 这里包含了模型的发送过程和取回过程。\n",
    "    results = await asyncio.gather(\n",
    "        *[\n",
    "            rwc.fit_model_on_worker(\n",
    "                worker=worker,\n",
    "                traced_model=traced_model,\n",
    "                batch_size=args.batch_size,\n",
    "                curr_round=curr_round,\n",
    "                max_nr_batches=args.federate_after_n_batches,\n",
    "                lr=learning_rate,\n",
    "            )\n",
    "            for worker in worker_instances\n",
    "        ]\n",
    "    )\n",
    "    models = {}\n",
    "    loss_values = {}\n",
    "    \n",
    "    # 每10轮进行一次test。使用test客户端检验当前结果的准确性。\n",
    "    # 这里主要测试，每个客户端发过来的模型的准确率。\n",
    "    test_models = curr_round % 10 == 1 or curr_round == args.training_rounds\n",
    "    if test_models:\n",
    "        logger.info(\"Evaluating models\")\n",
    "        np.set_printoptions(formatter={\"float\": \"{: .0f}\".format})\n",
    "        for worker_id, worker_model, _ in results:\n",
    "            rwc.evaluate_model_on_worker(\n",
    "                model_identifier=\"Model update \" + worker_id,\n",
    "                worker=testing,\n",
    "                dataset_key=\"mnist_testing\",\n",
    "                model=worker_model,\n",
    "                nr_bins=10,\n",
    "                batch_size=128,\n",
    "                print_target_hist=False,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "    # 将并行执行的多个客户端训练的结果，进行聚合。\n",
    "    for worker_id, worker_model, worker_loss in results:\n",
    "        if worker_model is not None:\n",
    "            models[worker_id] = worker_model\n",
    "            loss_values[worker_id] = worker_loss\n",
    "\n",
    "    # 调用联邦平均算法，对分布式models进行聚合。\n",
    "    traced_model = utils.federated_avg(models)\n",
    "\n",
    "    # 每10轮进行一次test。使用test客户端检验当前结果的准确性。\n",
    "    # 这里主要测试，模型聚合后，模型的准确率。\n",
    "    if test_models:\n",
    "        rwc.evaluate_model_on_worker(\n",
    "            model_identifier=\"Federated model\",\n",
    "            worker=testing,\n",
    "            dataset_key=\"mnist_testing\",\n",
    "            model=traced_model,\n",
    "            nr_bins=10,\n",
    "            batch_size=128,\n",
    "            print_target_hist=False,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    # decay learning rate\n",
    "    learning_rate = max(0.98 * learning_rate, args.lr * 0.01)\n",
    "\n",
    "if args.save_model:\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过40轮训练，我们在整个测试数据集上的准确率均达到95％以上。鉴于没有工人能使用超过4位数字，这给人留下了深刻的印象！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd057f55249afac9e3bb90b27c0916a1d44f0a08c86299e4ac4c83ac98b0a805cf4",
   "display_name": "Python 3.8.8 64-bit ('pysyft': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}