{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd057f55249afac9e3bb90b27c0916a1d44f0a08c86299e4ac4c83ac98b0a805cf4",
   "display_name": "Python 3.8.8 64-bit ('pysyft': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "57f55249afac9e3bb90b27c0916a1d44f0a08c86299e4ac4c83ac98b0a805cf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 联邦平均\n",
    "\n",
    "联邦学习（Federated Learning）是一种安全分布式深度学习技术，它允许各个数据持有者在不公开数据的情况下协同训练得到一个共享的模型，其目的是打破数据孤岛，在保护数据的隐私的前提下利用数据实现数据整合。\n",
    "目前关于联邦学习的实现有许多说法，有梯度聚合、模型平均、选择上传等等。有的认为参数服务器持有模型，参与者不持有；有的认为是各个数据持有者持有模型，参数服务器不需要获取模型。众说纷纭。\n",
    "但其核心是不变的：那就是数据分离，通信加密。\n",
    "\n",
    "联邦学习的各个参与者，会在本地训练模型，然后每一轮（或者固定间隔的轮次）将其模型参数，或者梯度（广义梯度，即前一轮次与当前轮次的模型参数的差）上传到参数服务器，由参数服务器将各个参与者的上传参数进行聚合，得到的结果再返还给各个参与者，参与者更新本地模型后，继续训练。\n",
    "在这个过程中，有如下几个计划：\n",
    "\n",
    "模型训练，模型是需要训练的，这个操作必须由各个参与者执行\n",
    "安全聚合，在梯度传递到参数服务器并返回给各个参与者这个过程中，传递的参数是不安全的，需要进行加密保护；并且，参数服务器要对参数进行聚合。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import syft as sy\n",
    "import copy\n",
    "hook = sy.TorchHook(torch)\n",
    "from torch import nn, optim"
   ]
  },
  {
   "source": [
    "## 步骤1 创建数据的所有者"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a couple workers\n",
    "\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "secure_worker = sy.VirtualWorker(hook, id=\"secure_worker\")\n",
    "\n",
    "\n",
    "# A Toy Dataset\n",
    "data = torch.tensor([[0,0],[0,1],[1,0],[1,1.]], requires_grad=True)\n",
    "target = torch.tensor([[0],[0],[1],[1.]], requires_grad=True)\n",
    "\n",
    "# get pointers to training data on each worker by\n",
    "# sending some training data to bob and alice\n",
    "bobs_data = data[0:2].send(bob)\n",
    "bobs_target = target[0:2].send(bob)\n",
    "\n",
    "alices_data = data[2:].send(alice)\n",
    "alices_target = target[2:].send(alice)"
   ]
  },
  {
   "source": [
    "## 步骤2 创建模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniitalize A Toy Model\n",
    "model = nn.Linear(2,1)"
   ]
  },
  {
   "source": [
    "## 步骤3 将模型发送给Alice和Bob\n",
    "\n",
    "> 1. 创建模型，前项传播计算中间结果，计算损失值，损失值反向传播形成梯度，使用优化器进行梯度下降。\n",
    "> 2. 在联邦学习模型中，聚合的是模型的权重，而不是模型的梯度。是经过一次梯度下降操作后的模型的权重。\n",
    "> 3. adam优化器中的参数包括两个部分：一个是动量梯度计算，一个是RMscrop。都是由历史的梯度得出来的。\n",
    "> 4. 所以要想服务器知道模型的adam优化器的结果，必须将历史的梯度也进行上传，显然不合理。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bobs_model = model.copy().send(bob)\n",
    "alices_model = model.copy().send(alice)\n",
    "\n",
    "bobs_opt = optim.SGD(params=bobs_model.parameters(),lr=0.1)\n",
    "alices_opt = optim.SGD(params=alices_model.parameters(),lr=0.1)"
   ]
  },
  {
   "source": [
    "## 步骤4 并行训练Bob和Alice的模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bob:tensor(0.1430) Alice:tensor(1.3841)\nBob:tensor(0.0712) Alice:tensor(0.0938)\nBob:tensor(0.0492) Alice:tensor(0.0699)\nBob:tensor(0.0394) Alice:tensor(0.0582)\nBob:tensor(0.0330) Alice:tensor(0.0484)\nBob:tensor(0.0280) Alice:tensor(0.0403)\nBob:tensor(0.0239) Alice:tensor(0.0335)\nBob:tensor(0.0204) Alice:tensor(0.0279)\nBob:tensor(0.0174) Alice:tensor(0.0232)\nBob:tensor(0.0148) Alice:tensor(0.0193)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\n",
    "    # Train Bob's Model\n",
    "    bobs_opt.zero_grad()\n",
    "    bobs_pred = bobs_model(bobs_data)\n",
    "    bobs_loss = ((bobs_pred - bobs_target)**2).sum()\n",
    "    bobs_loss.backward()\n",
    "\n",
    "    bobs_opt.step()\n",
    "    bobs_loss = bobs_loss.get().data\n",
    "\n",
    "    # Train Alice's Model\n",
    "    alices_opt.zero_grad()\n",
    "    alices_pred = alices_model(alices_data)\n",
    "    alices_loss = ((alices_pred - alices_target)**2).sum()\n",
    "    alices_loss.backward()\n",
    "\n",
    "    alices_opt.step()\n",
    "    alices_loss = alices_loss.get().data\n",
    "    \n",
    "    print(\"Bob:\" + str(bobs_loss) + \" Alice:\" + str(alices_loss))"
   ]
  },
  {
   "source": [
    "## 步骤5 客户端发送模型到服务器"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alices_model.move(secure_worker)\n",
    "bobs_model.move(secure_worker)"
   ]
  },
  {
   "source": [
    "## 步骤6 模型平均"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.weight.set_(((alices_model.weight.data + bobs_model.weight.data) / 2).get())\n",
    "    model.bias.set_(((alices_model.bias.data + bobs_model.bias.data) / 2).get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([('weight', Parameter containing:\ntensor([[0.6258, 0.1664]], requires_grad=True)), ('bias', Parameter containing:\ntensor([-0.0167], requires_grad=True))])\n"
     ]
    }
   ],
   "source": [
    "print(model._parameters)"
   ]
  },
  {
   "source": [
    "## 步骤7 迭代以上步骤"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bob:tensor(0.0046) Alice:tensor(0.0139)\n",
      "Bob:tensor(0.0013) Alice:tensor(0.0066)\n",
      "Bob:tensor(0.0003) Alice:tensor(0.0030)\n",
      "Bob:tensor(4.8089e-05) Alice:tensor(0.0014)\n",
      "Bob:tensor(4.9928e-05) Alice:tensor(0.0006)\n",
      "Bob:tensor(9.4057e-05) Alice:tensor(0.0003)\n",
      "Bob:tensor(0.0001) Alice:tensor(0.0001)\n",
      "Bob:tensor(0.0001) Alice:tensor(7.4250e-05)\n",
      "Bob:tensor(0.0001) Alice:tensor(3.8461e-05)\n",
      "Bob:tensor(0.0001) Alice:tensor(2.0683e-05)\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "worker_iters = 5\n",
    "\n",
    "for a_iter in range(iterations):\n",
    "    \n",
    "    bobs_model = model.copy().send(bob)\n",
    "    alices_model = model.copy().send(alice)\n",
    "\n",
    "    bobs_opt = optim.SGD(params=bobs_model.parameters(),lr=0.1)\n",
    "    alices_opt = optim.SGD(params=alices_model.parameters(),lr=0.1)\n",
    "\n",
    "    for wi in range(worker_iters):\n",
    "\n",
    "        # Train Bob's Model\n",
    "        bobs_opt.zero_grad()\n",
    "        bobs_pred = bobs_model(bobs_data)\n",
    "        bobs_loss = ((bobs_pred - bobs_target)**2).sum()\n",
    "        bobs_loss.backward()\n",
    "\n",
    "        bobs_opt.step()\n",
    "        bobs_loss = bobs_loss.get().data\n",
    "\n",
    "        # Train Alice's Model\n",
    "        alices_opt.zero_grad()\n",
    "        alices_pred = alices_model(alices_data)\n",
    "        alices_loss = ((alices_pred - alices_target)**2).sum()\n",
    "        alices_loss.backward()\n",
    "\n",
    "        alices_opt.step()\n",
    "        alices_loss = alices_loss.get().data\n",
    "    \n",
    "    alices_model.move(secure_worker)\n",
    "    bobs_model.move(secure_worker)\n",
    "    with torch.no_grad():\n",
    "        model.weight.set_(((alices_model.weight.data + bobs_model.weight.data) / 2).get())\n",
    "        model.bias.set_(((alices_model.bias.data + bobs_model.bias.data) / 2).get())\n",
    "    \n",
    "    print(\"Bob:\" + str(bobs_loss) + \" Alice:\" + str(alices_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证模型\n",
    "preds = model(data)\n",
    "loss = ((preds - target) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.0356],\n        [0.0321],\n        [0.9589],\n        [0.9553]], grad_fn=<AddmmBackward>)\ntensor([[0.],\n        [0.],\n        [1.],\n        [1.]], requires_grad=True)\ntensor(0.0060)\n"
     ]
    }
   ],
   "source": [
    "print(preds)\n",
    "print(target)\n",
    "print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}