{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd057f55249afac9e3bb90b27c0916a1d44f0a08c86299e4ac4c83ac98b0a805cf4",
   "display_name": "Python 3.8.8 64-bit ('pysyft': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "57f55249afac9e3bb90b27c0916a1d44f0a08c86299e4ac4c83ac98b0a805cf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 安全训练评估\n",
    "\n",
    "在构建机器学习即服务解决方案（MLaaS）时，公司可能需要请求其他合作伙伴访问数据以训练其模型。在卫生或金融领域，模型和数据都非常关键：模型参数是业务资产，而数据是严格监管的个人数据。\n",
    "\n",
    "在这种情况下，一种可能的解决方案是对模型和数据都进行加密，并在加密后的值上训练机器学习模型。例如，这保证了公司不会访问患者的病历，并且医疗机构将无法观察他们所贡献的模型。存在几种允许对加密数据进行计算的加密方案，其中包括安全多方计算（SMPC），同态加密（FHE / SHE）和功能加密（FE）。我们将在这里集中讨论多方计算（已在教程5中进行了介绍），它由私有加性共享组成，并依赖于加密协议SecureNN和SPDZ。\n",
    "\n",
    "本教程的确切设置如下：考虑您是服务器，并且您想对模型中的某些数据进行训练。  𝑛 工人。服务器机密共享他的模型，并将每个共享发送给工作人员。工人们还秘密共享他们的数据并在他们之间交换数据。在我们将要研究的配置中，有2个工人：alice和bob。交换股份后，他们每个人现在拥有自己的股份，另一工人的股份和模型的股份。现在，计算可以开始使用适当的加密协议对模型进行私下训练。训练模型后，所有份额都可以发送回服务器以对其进行解密。下图对此进行了说明："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "# We don't use the whole dataset for efficiency purpose, but feel free to increase these numbers\n",
    "n_train_items = 640\n",
    "n_test_items = 640"
   ]
  },
  {
   "source": [
    "## 1 导入与配置"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 64\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.02\n",
    "        self.seed = 1\n",
    "        self.log_interval = 1 # Log info at each batch\n",
    "        self.precision_fractional = 3\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "_ = torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy  # import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # hook PyTorch to add extra functionalities like Federated and Encrypted Learning\n",
    "\n",
    "# simulation functions\n",
    "def connect_to_workers(n_workers):\n",
    "    return [\n",
    "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
    "        for i in range(n_workers)\n",
    "    ]\n",
    "def connect_to_crypto_provider():\n",
    "    return sy.VirtualWorker(hook, id=\"crypto_provider\")\n",
    "\n",
    "workers = connect_to_workers(n_workers=2)\n",
    "crypto_provider = connect_to_crypto_provider()"
   ]
  },
  {
   "source": [
    "## 2 秘密共享数据\n",
    "在这里，我们使用一个效用函数来模拟以下行为：我们假设MNIST数据集分布在各个部分中，每个部分都由我们的一个工人持有。然后，工作人员将其数据分批拆分，并在彼此之间秘密共享其数据。返回的最终对象是这些秘密共享批次上的可迭代对象，我们将其称为私有数据加载器。请注意，在此过程中，本地工作人员（因此我们）从未访问过数据。\n",
    "\n",
    "我们像往常一样获得了训练和测试私有数据集，并且输入和标签都是秘密共享的。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_private_data_loaders(precision_fractional, workers, crypto_provider):\n",
    "    \n",
    "    def one_hot_of(index_tensor):\n",
    "        \"\"\"\n",
    "        Transform to one hot tensor\n",
    "        \n",
    "        Example:\n",
    "            [0, 3, 9]\n",
    "            =>\n",
    "            [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "             [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "             [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]\n",
    "            \n",
    "        \"\"\"\n",
    "        onehot_tensor = torch.zeros(*index_tensor.shape, 10) # 10 classes for MNIST\n",
    "        onehot_tensor = onehot_tensor.scatter(1, index_tensor.view(-1, 1), 1)\n",
    "        return onehot_tensor\n",
    "        \n",
    "    def secret_share(tensor):\n",
    "        \"\"\"\n",
    "        Transform to fixed precision and secret share a tensor\n",
    "        \"\"\"\n",
    "        return (\n",
    "            tensor\n",
    "            .fix_precision(precision_fractional=precision_fractional)\n",
    "            .share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "        )\n",
    "    \n",
    "    transformation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=True, download=True, transform=transformation),\n",
    "        batch_size=args.batch_size\n",
    "    )\n",
    "    \n",
    "    private_train_loader = [\n",
    "        (secret_share(data), secret_share(one_hot_of(target)))\n",
    "        for i, (data, target) in enumerate(train_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, download=True, transform=transformation),\n",
    "        batch_size=args.test_batch_size\n",
    "    )\n",
    "    \n",
    "    private_test_loader = [\n",
    "        (secret_share(data), secret_share(target.float()))\n",
    "        for i, (data, target) in enumerate(test_loader)\n",
    "        if i < n_test_items / args.test_batch_size\n",
    "    ]\n",
    "    \n",
    "    return private_train_loader, private_test_loader\n",
    "    \n",
    "    \n",
    "private_train_loader, private_test_loader = get_private_data_loaders(\n",
    "    precision_fractional=args.precision_fractional,\n",
    "    workers=workers,\n",
    "    crypto_provider=crypto_provider\n",
    ")"
   ]
  },
  {
   "source": [
    "## 3 实现模型\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "source": [
    "## 4 训练和测试"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, private_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(private_train_loader): # <-- now it is a private dataset\n",
    "        start_time = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        # loss = F.nll_loss(output, target)  <-- not possible here\n",
    "        batch_size = output.shape[0]\n",
    "        loss = ((output - target)**2).sum().refresh()/batch_size\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get().float_precision()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.3f}s'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(private_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(private_train_loader), loss.item(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, private_test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in private_test_loader:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "    correct = correct.get().float_precision()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct.item(), len(private_test_loader)* args.test_batch_size,\n",
    "        100. * correct.item() / (len(private_test_loader) * args.test_batch_size)))"
   ]
  },
  {
   "source": [
    "## 5 训练"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: 1 [0/640 (0%)]\tLoss: 1.128000\tTime: 3.911s\n",
      "Train Epoch: 1 [64/640 (10%)]\tLoss: 1.012000\tTime: 3.977s\n",
      "Train Epoch: 1 [128/640 (20%)]\tLoss: 0.989000\tTime: 4.051s\n",
      "Train Epoch: 1 [192/640 (30%)]\tLoss: 0.902000\tTime: 4.048s\n",
      "Train Epoch: 1 [256/640 (40%)]\tLoss: 0.888000\tTime: 3.872s\n",
      "Train Epoch: 1 [320/640 (50%)]\tLoss: 0.876000\tTime: 3.966s\n",
      "Train Epoch: 1 [384/640 (60%)]\tLoss: 0.854000\tTime: 3.994s\n",
      "Train Epoch: 1 [448/640 (70%)]\tLoss: 0.853000\tTime: 4.016s\n",
      "Train Epoch: 1 [512/640 (80%)]\tLoss: 0.829000\tTime: 4.067s\n",
      "Train Epoch: 1 [576/640 (90%)]\tLoss: 0.841000\tTime: 4.133s\n",
      "\n",
      "Test set: Accuracy: 227.0/640 (35%)\n",
      "\n",
      "Train Epoch: 2 [0/640 (0%)]\tLoss: 0.781000\tTime: 3.979s\n",
      "Train Epoch: 2 [64/640 (10%)]\tLoss: 0.733000\tTime: 3.990s\n",
      "Train Epoch: 2 [128/640 (20%)]\tLoss: 0.791000\tTime: 4.032s\n",
      "Train Epoch: 2 [192/640 (30%)]\tLoss: 0.717000\tTime: 4.037s\n",
      "Train Epoch: 2 [256/640 (40%)]\tLoss: 0.707000\tTime: 4.151s\n",
      "Train Epoch: 2 [320/640 (50%)]\tLoss: 0.706000\tTime: 3.998s\n",
      "Train Epoch: 2 [384/640 (60%)]\tLoss: 0.709000\tTime: 4.048s\n",
      "Train Epoch: 2 [448/640 (70%)]\tLoss: 0.721000\tTime: 4.199s\n",
      "Train Epoch: 2 [512/640 (80%)]\tLoss: 0.710000\tTime: 4.113s\n",
      "Train Epoch: 2 [576/640 (90%)]\tLoss: 0.743000\tTime: 4.070s\n",
      "\n",
      "Test set: Accuracy: 360.0/640 (56%)\n",
      "\n",
      "Train Epoch: 3 [0/640 (0%)]\tLoss: 0.667000\tTime: 3.908s\n",
      "Train Epoch: 3 [64/640 (10%)]\tLoss: 0.596000\tTime: 3.894s\n",
      "Train Epoch: 3 [128/640 (20%)]\tLoss: 0.692000\tTime: 3.958s\n",
      "Train Epoch: 3 [192/640 (30%)]\tLoss: 0.600000\tTime: 3.913s\n",
      "Train Epoch: 3 [256/640 (40%)]\tLoss: 0.589000\tTime: 3.933s\n",
      "Train Epoch: 3 [320/640 (50%)]\tLoss: 0.590000\tTime: 3.900s\n",
      "Train Epoch: 3 [384/640 (60%)]\tLoss: 0.606000\tTime: 3.946s\n",
      "Train Epoch: 3 [448/640 (70%)]\tLoss: 0.628000\tTime: 4.041s\n",
      "Train Epoch: 3 [512/640 (80%)]\tLoss: 0.619000\tTime: 3.918s\n",
      "Train Epoch: 3 [576/640 (90%)]\tLoss: 0.668000\tTime: 3.974s\n",
      "\n",
      "Test set: Accuracy: 401.0/640 (63%)\n",
      "\n",
      "Train Epoch: 4 [0/640 (0%)]\tLoss: 0.584000\tTime: 3.958s\n",
      "Train Epoch: 4 [64/640 (10%)]\tLoss: 0.499000\tTime: 4.126s\n",
      "Train Epoch: 4 [128/640 (20%)]\tLoss: 0.618000\tTime: 3.908s\n",
      "Train Epoch: 4 [192/640 (30%)]\tLoss: 0.518000\tTime: 3.876s\n",
      "Train Epoch: 4 [256/640 (40%)]\tLoss: 0.512000\tTime: 3.933s\n",
      "Train Epoch: 4 [320/640 (50%)]\tLoss: 0.511000\tTime: 3.976s\n",
      "Train Epoch: 4 [384/640 (60%)]\tLoss: 0.535000\tTime: 3.972s\n",
      "Train Epoch: 4 [448/640 (70%)]\tLoss: 0.562000\tTime: 3.889s\n",
      "Train Epoch: 4 [512/640 (80%)]\tLoss: 0.552000\tTime: 3.968s\n",
      "Train Epoch: 4 [576/640 (90%)]\tLoss: 0.611000\tTime: 4.015s\n",
      "\n",
      "Test set: Accuracy: 424.0/640 (66%)\n",
      "\n",
      "Train Epoch: 5 [0/640 (0%)]\tLoss: 0.525000\tTime: 4.026s\n",
      "Train Epoch: 5 [64/640 (10%)]\tLoss: 0.435000\tTime: 4.066s\n",
      "Train Epoch: 5 [128/640 (20%)]\tLoss: 0.559000\tTime: 4.073s\n",
      "Train Epoch: 5 [192/640 (30%)]\tLoss: 0.459000\tTime: 4.088s\n",
      "Train Epoch: 5 [256/640 (40%)]\tLoss: 0.454000\tTime: 4.103s\n",
      "Train Epoch: 5 [320/640 (50%)]\tLoss: 0.451000\tTime: 4.087s\n",
      "Train Epoch: 5 [384/640 (60%)]\tLoss: 0.480000\tTime: 4.112s\n",
      "Train Epoch: 5 [448/640 (70%)]\tLoss: 0.510000\tTime: 4.130s\n",
      "Train Epoch: 5 [512/640 (80%)]\tLoss: 0.501000\tTime: 4.170s\n",
      "Train Epoch: 5 [576/640 (90%)]\tLoss: 0.567000\tTime: 4.097s\n",
      "\n",
      "Test set: Accuracy: 449.0/640 (70%)\n",
      "\n",
      "Train Epoch: 6 [0/640 (0%)]\tLoss: 0.476000\tTime: 4.153s\n",
      "Train Epoch: 6 [64/640 (10%)]\tLoss: 0.387000\tTime: 4.176s\n",
      "Train Epoch: 6 [128/640 (20%)]\tLoss: 0.516000\tTime: 4.239s\n",
      "Train Epoch: 6 [192/640 (30%)]\tLoss: 0.410000\tTime: 4.286s\n",
      "Train Epoch: 6 [256/640 (40%)]\tLoss: 0.412000\tTime: 4.359s\n",
      "Train Epoch: 6 [320/640 (50%)]\tLoss: 0.406000\tTime: 4.303s\n",
      "Train Epoch: 6 [384/640 (60%)]\tLoss: 0.438000\tTime: 4.292s\n",
      "Train Epoch: 6 [448/640 (70%)]\tLoss: 0.471000\tTime: 4.288s\n",
      "Train Epoch: 6 [512/640 (80%)]\tLoss: 0.462000\tTime: 4.347s\n",
      "Train Epoch: 6 [576/640 (90%)]\tLoss: 0.529000\tTime: 4.359s\n",
      "\n",
      "Test set: Accuracy: 464.0/640 (72%)\n",
      "\n",
      "Train Epoch: 7 [0/640 (0%)]\tLoss: 0.434000\tTime: 4.554s\n",
      "Train Epoch: 7 [64/640 (10%)]\tLoss: 0.352000\tTime: 4.660s\n",
      "Train Epoch: 7 [128/640 (20%)]\tLoss: 0.476000\tTime: 4.629s\n",
      "Train Epoch: 7 [192/640 (30%)]\tLoss: 0.378000\tTime: 4.719s\n",
      "Train Epoch: 7 [256/640 (40%)]\tLoss: 0.375000\tTime: 4.860s\n",
      "Train Epoch: 7 [320/640 (50%)]\tLoss: 0.368000\tTime: 4.745s\n",
      "Train Epoch: 7 [384/640 (60%)]\tLoss: 0.403000\tTime: 4.513s\n",
      "Train Epoch: 7 [448/640 (70%)]\tLoss: 0.440000\tTime: 4.559s\n",
      "Train Epoch: 7 [512/640 (80%)]\tLoss: 0.428000\tTime: 4.649s\n",
      "Train Epoch: 7 [576/640 (90%)]\tLoss: 0.499000\tTime: 4.660s\n",
      "\n",
      "Test set: Accuracy: 469.0/640 (73%)\n",
      "\n",
      "Train Epoch: 8 [0/640 (0%)]\tLoss: 0.407000\tTime: 4.558s\n",
      "Train Epoch: 8 [64/640 (10%)]\tLoss: 0.323000\tTime: 4.625s\n",
      "Train Epoch: 8 [128/640 (20%)]\tLoss: 0.447000\tTime: 4.692s\n",
      "Train Epoch: 8 [192/640 (30%)]\tLoss: 0.349000\tTime: 5.024s\n",
      "Train Epoch: 8 [256/640 (40%)]\tLoss: 0.348000\tTime: 4.977s\n",
      "Train Epoch: 8 [320/640 (50%)]\tLoss: 0.342000\tTime: 4.871s\n",
      "Train Epoch: 8 [384/640 (60%)]\tLoss: 0.375000\tTime: 4.719s\n",
      "Train Epoch: 8 [448/640 (70%)]\tLoss: 0.411000\tTime: 4.706s\n",
      "Train Epoch: 8 [512/640 (80%)]\tLoss: 0.403000\tTime: 4.809s\n",
      "Train Epoch: 8 [576/640 (90%)]\tLoss: 0.475000\tTime: 4.738s\n",
      "\n",
      "Test set: Accuracy: 474.0/640 (74%)\n",
      "\n",
      "Train Epoch: 9 [0/640 (0%)]\tLoss: 0.384000\tTime: 4.954s\n",
      "Train Epoch: 9 [64/640 (10%)]\tLoss: 0.301000\tTime: 5.081s\n",
      "Train Epoch: 9 [128/640 (20%)]\tLoss: 0.421000\tTime: 5.052s\n",
      "Train Epoch: 9 [192/640 (30%)]\tLoss: 0.327000\tTime: 5.100s\n",
      "Train Epoch: 9 [256/640 (40%)]\tLoss: 0.325000\tTime: 5.167s\n",
      "Train Epoch: 9 [320/640 (50%)]\tLoss: 0.318000\tTime: 5.194s\n",
      "Train Epoch: 9 [384/640 (60%)]\tLoss: 0.353000\tTime: 5.207s\n",
      "Train Epoch: 9 [448/640 (70%)]\tLoss: 0.391000\tTime: 5.322s\n",
      "Train Epoch: 9 [512/640 (80%)]\tLoss: 0.379000\tTime: 5.285s\n",
      "Train Epoch: 9 [576/640 (90%)]\tLoss: 0.455000\tTime: 5.268s\n",
      "\n",
      "Test set: Accuracy: 481.0/640 (75%)\n",
      "\n",
      "Train Epoch: 10 [0/640 (0%)]\tLoss: 0.363000\tTime: 5.530s\n",
      "Train Epoch: 10 [64/640 (10%)]\tLoss: 0.281000\tTime: 5.555s\n",
      "Train Epoch: 10 [128/640 (20%)]\tLoss: 0.398000\tTime: 6.025s\n",
      "Train Epoch: 10 [192/640 (30%)]\tLoss: 0.306000\tTime: 5.286s\n",
      "Train Epoch: 10 [256/640 (40%)]\tLoss: 0.306000\tTime: 5.300s\n",
      "Train Epoch: 10 [320/640 (50%)]\tLoss: 0.296000\tTime: 5.410s\n",
      "Train Epoch: 10 [384/640 (60%)]\tLoss: 0.332000\tTime: 5.457s\n",
      "Train Epoch: 10 [448/640 (70%)]\tLoss: 0.371000\tTime: 5.527s\n",
      "Train Epoch: 10 [512/640 (80%)]\tLoss: 0.356000\tTime: 5.646s\n",
      "Train Epoch: 10 [576/640 (90%)]\tLoss: 0.435000\tTime: 5.506s\n",
      "\n",
      "Test set: Accuracy: 488.0/640 (76%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model = model.fix_precision().share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "optimizer = optimizer.fix_precision() \n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, private_train_loader, optimizer, epoch)\n",
    "    test(args, model, private_test_loader)"
   ]
  },
  {
   "source": [
    "## 6 相关讨论\n",
    "\n",
    "## 6.1计算时间\n",
    "\n",
    "第一件事显然是运行时间！您肯定已经注意到，它比纯文本训练要慢得多。特别是，在1批64项上进行一次迭代需要3.2 s，而在纯PyTorch中只有13 ms。尽管这似乎是一个阻止程序，但请回想一下，这里的所有事情都是远程发生的，并且是在加密的世界中发生的：没有单个数据项被公开。更具体地说，处理一项的时间为50ms，这还不错。真正的问题是分析何时需要加密训练以及何时仅加密预测就足够了。例如，在生产就绪的情况下，完全可以接受50毫秒执行预测！\n",
    "\n",
    "一个主要的瓶颈是昂贵的激活功能的使用：SMPC的relu激活非常昂贵，因为它使用私有比较和SecureNN协议。举例说明，如果我们用二次激活代替relu，就像在CryptoNets等加密计算的几篇论文中所做的那样，我们将从3.2s降到1.2s。\n",
    "\n",
    "通常，要删除的关键思想是仅加密必要的内容，本教程向您展示了它的简单性\n",
    "\n",
    "## 6.2使用SMPC进行反向传播\n",
    "您可能想知道我们如何执行反向传播和梯度更新，尽管我们正在有限域中使用整数。为此，我们开发了一个新的syft张量，称为AutogradTensor。尽管您可能还没有看过本教程，但它还是大量使用它！让我们通过打印模型的重量进行检查：\n",
    "\n",
    "## 6.3安全保障\n",
    "最后，让我们给出一些有关我们在此处实现的安全性的提示：我们在这里考虑的对手是诚实但好奇的：这意味着对手无法通过运行此协议来学习有关数据的任何信息，但是恶意的对手可以仍然偏离协议，例如尝试破坏共享以破坏计算。在此类SMPC计算（包括私有比较）中针对恶意对手的安全性仍然是一个未解决的问题。\n",
    "\n",
    "此外，即使“安全多方计算”确保不访问培训数据，此处仍然存在来自纯文本世界的许多威胁。例如，当您可以向模型提出请求时（在MLaaS的上下文中），您可以获得可能泄露有关训练数据集信息的预测。特别是，您没有针对成员资格攻击的任何保护措施，这是对机器学习服务的常见攻击，在这种攻击中，对手要确定是否在数据集中使用了特定项目。除此之外，其他攻击，例如意外的记忆过程（模型学习有关数据项的特定特征的模型），模型倒置或提取，仍然是可能的。\n",
    "\n",
    "对上述许多威胁有效的一种通用解决方案是添加差异隐私。它可以与安全的多方计算完美地结合在一起，并且可以提供非常有趣的安全性保证。我们目前正在研究几种实现方式，并希望提出一个将两者结合起来的示例！"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}