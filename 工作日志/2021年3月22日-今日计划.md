## 计划

1. 每天sklearn一个机器学习小算法
   - [x] 支持向量机（学习原理、sklearn实践
   - [x] 集成方法（学习原理、sklearn实践
2. 每天一个联邦学习框架
   - [ ] pysyft学习


## 收获

1. 对分类的所有方法包括原理和基本的原理实现numpy进行了总结。主要参考机器学习实践。主要包括以下六种分类方法。没有更多了。这里的机器学习算法，既包括模型，也包括模型的训练算法。
   1. k近邻：基于邻居的机器学习算法：KNN 
   2. 决策树：基于树和信息增益的机器学习算法：DT
   3. 朴素贝叶斯：基于贝叶斯定力的机器学习算法：NB 
   4. 逻辑回归：基于回归的方式处理分类问题：
   5. 支持向量机：基于支持向量的机器学习算法：
   6. 集成方法：基于两种不同的集成方式。bagging和boosting。
2. 对sklearn进行了学习。
   1. 主要学习了sklearn的算法核心。这些算法不是单纯的以分类和回归两个类别来进行分类的。而是以算法的核心思想来进行分类的。比如基于邻居的既可以聚类也可以分类，基于树的可以分类也可以回归。即以算法的思想进行分包。而不是以算法处理的问题类别。包括neighbors/tree/naive_bayes/linear_model/svm/esemble六个机器学习核心。
      1. neighbors在第二章基于邻居的方法中有说明sklearn.neighbors.KNNClassifier
      2. tree第四章前两节 sklearn.tree.DeceisionTreeClassifier
      3. naive_bayes第四章10节sklearn.naive_bayes.GaussianNB/MultinomialNB/BernoulliNB
      4. linear_model.logisticRegression第三章第7节sgdclassier第9节。sklearn.linear_model.logisticRegression
      5. svm第四章第5节sklearn.svm.SVC
      6. esemeble第四章34节只有randomforest。sklearn.esemble.RandomForestClassifier/AdaBoost
   2. 区分了随机梯度下降算法和以上方法的区别。以上方法都有其模型算法的核心。并非随机梯度下降。在linear_model.SGDClassifier中以logisticsRegression和SVM等算法为核心。进行梯度下降算法。梯度下降算法是一种模型训练算法，而非模型本身。默认是用SVM进行梯度下降，主要解决数据过大的问题。linear_model.SGDRegression主要是用SGD来处理回归问题。
   3. 关于多分类问题的处理。主要包括两种方式onevsone和onevsall两种模式。sklearn.multiclass.OneVsRestClassifier/OneVsOneClassifier
3. 学习了模型预处理和评估的方法。主要包括以下两部分
   1. sklearn中的Dataset加载和使用。sklear.datasets
   2. 数据的标准化和归一化（正则标准化和minmax标准化）sklearn.procession.minmax_scale/normalize
   3. 处理数据缺失值的方法sklearn.impute.kNNimpute...
   4. 构建词向量的方法。可以一键构建词向量。sklearn.feature_extraction.from_graph/from_text
   5. 数据降维的方法：PCA、SVD、因子分析法sklearn.decomposition.PCA/SVD
   6. 数据处理流的方法。sklearn.Pipeline.pipeline
   7. 模型评估的方法sklearn.metrics.roc/auc/f1_score
   8. 关于半监督的处理方式。主要包括标签传递算法等。sklearn.semi_supervised
   9. 模糊机器学习算法。让数据在数据周围进行微小的扰动。添加高斯模糊或者bayes模糊。sklearn.mixture
   10. 模型选择的方法。交叉验证sklearn.modelselection.kfold
   11. 特征选择的方法。sklearn.feature_selectiron.*
